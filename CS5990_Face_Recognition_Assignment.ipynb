{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS5990_Face_Recognition_Assignment",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0LTkJTwm71KA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CS5990: Face Recognition Assignment\n",
        "### Overview:\n",
        "You will be submitting images of people onto Google Drive for recognition.  The library imported from https://github.com/ageitgey/face_recognition and has been formatted onto Google Colab for your convenience.  Please follow through each code cell and read through the comments if you wish to gain a better understanding of how it works.  Otherwise, simply follow the instructions indicated through comments.\n",
        "\n",
        "## What you will need:\n",
        "1. Images of person(s)\n",
        "2. Another image in which you want to identify these person(s)\n",
        "\n",
        "Be sure to save these somewhere in your Google Drive and remember the location for later.\n",
        "\n",
        "\n",
        "##Steps:  \n",
        "[1] Import & install libraries and preprocessing functions.  This step requires you to execute all cells until step [2].  \n",
        "[2] Mount to your Google drive where your images are saved.  \n",
        "[3] Modify the code to point to the images with the proper names in your Drive.  Execute and observe output."
      ]
    },
    {
      "metadata": {
        "id": "M9kjNrj48mUh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### [1] We begin by installing the necessary dependencies for image processing, face detection, and pre-trained models."
      ]
    },
    {
      "metadata": {
        "id": "kgtmnGf9YoSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install face_recognition_models\n",
        "!pip install dlib\n",
        "!pip install opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV32ANBC9TaG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the installed libraries.\n",
        "#### We also begin importing dlib's pre-trained face detection models as well as pre-trained face_recognition models that produce the 128-dimensional vectors mentioned in the presentation."
      ]
    },
    {
      "metadata": {
        "id": "y9YdVKVCYqx4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "import dlib\n",
        "import numpy as np\n",
        "import face_recognition_models\n",
        "\n",
        "\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "predictor_68_point_model = face_recognition_models.pose_predictor_model_location()\n",
        "pose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model)\n",
        "\n",
        "predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\n",
        "pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\n",
        "\n",
        "cnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\n",
        "cnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model)\n",
        "\n",
        "face_recognition_model = face_recognition_models.face_recognition_model_location()\n",
        "face_encoder = dlib.face_recognition_model_v1(face_recognition_model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHXwf-LM-A1P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Euclidean Distance (L2) of two 128-d vectors"
      ]
    },
    {
      "metadata": {
        "id": "Iegb2cLW9FkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def face_distance(face_encodings, face_to_compare):\n",
        "    \"\"\"\n",
        "    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n",
        "    for each comparison face. The distance tells you how similar the faces are.\n",
        "\n",
        "    :param faces: List of face encodings to compare\n",
        "    :param face_to_compare: A face encoding to compare against\n",
        "    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n",
        "    \"\"\"\n",
        "    if len(face_encodings) == 0:\n",
        "        return np.empty((0))\n",
        "\n",
        "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
        "  \n",
        "def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.55):\n",
        "    \"\"\"\n",
        "    Compare a list of face encodings against a candidate encoding to see if they match.\n",
        "\n",
        "    :param known_face_encodings: A list of known face encodings\n",
        "    :param face_encoding_to_check: A single face encoding to compare against the list\n",
        "    :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\n",
        "    :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\n",
        "    \"\"\"\n",
        "    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OIt9lkRA-HpD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert image to numpy array"
      ]
    },
    {
      "metadata": {
        "id": "RKljTjyy9C37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_file(file, mode='RGB'):\n",
        "    \"\"\"\n",
        "    Loads an image file (.jpg, .png, etc) into a numpy array\n",
        "\n",
        "    :param file: image file name or file object to load\n",
        "    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n",
        "    :return: image contents as numpy array\n",
        "    \"\"\"\n",
        "    im = PIL.Image.open(file)\n",
        "    if mode:\n",
        "        im = im.convert(mode)\n",
        "    return np.array(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_ajggKz-RIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Methods that return bounding boxes of faces, can utilize the different face detection models mentioned:\n",
        "1. Histogram of Oriented Gradients (HOG) [DEFAULT]\n",
        "2. CNN"
      ]
    },
    {
      "metadata": {
        "id": "S8lbI06k5kYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
        "    \"\"\"\n",
        "    Returns an array of bounding boxes of human faces in a image\n",
        "\n",
        "    :param img: An image (as a numpy array)\n",
        "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
        "    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n",
        "                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n",
        "    :return: A list of dlib 'rect' objects of found face locations\n",
        "    \"\"\"\n",
        "    if model == \"cnn\":\n",
        "        return cnn_face_detector(img, number_of_times_to_upsample)\n",
        "    else:\n",
        "        return face_detector(img, number_of_times_to_upsample)\n",
        "\n",
        "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
        "    \"\"\"\n",
        "    Returns an array of bounding boxes of human faces in a image\n",
        "\n",
        "    :param img: An image (as a numpy array)\n",
        "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
        "    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n",
        "                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n",
        "    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n",
        "    \"\"\"\n",
        "    if model == \"cnn\":\n",
        "        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n",
        "    else:\n",
        "        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\n",
        "\n",
        "def _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\n",
        "    \"\"\"\n",
        "    Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\n",
        "\n",
        "    :param img: A list of images (each as a numpy array)\n",
        "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
        "    :return: A list of dlib 'rect' objects of found face locations\n",
        "    \"\"\"\n",
        "    return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)\n",
        "  \n",
        "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n",
        "    \"\"\"\n",
        "    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n",
        "    If you are using a GPU, this can give you much faster results since the GPU\n",
        "    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n",
        "\n",
        "    :param img: A list of images (each as a numpy array)\n",
        "    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n",
        "    :param batch_size: How many images to include in each GPU processing batch.\n",
        "    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n",
        "    \"\"\"\n",
        "    def convert_cnn_detections_to_css(detections):\n",
        "        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n",
        "\n",
        "    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n",
        "\n",
        "    return list(map(convert_cnn_detections_to_css, raw_detections_batched))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3dUKdHQ-ypI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Methods Obtain coordinates of facial landmarks\n",
        "These landmarks can be used for many different tasks including:\n",
        "1. Facial Recognition\n",
        "2. Emotion Classification\n",
        "3. Digital Makeup"
      ]
    },
    {
      "metadata": {
        "id": "O9gGlTNr5Xny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
        "    if face_locations is None:\n",
        "        face_locations = _raw_face_locations(face_image)\n",
        "    else:\n",
        "        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\n",
        "\n",
        "    pose_predictor = pose_predictor_68_point\n",
        "\n",
        "    if model == \"small\":\n",
        "        pose_predictor = pose_predictor_5_point\n",
        "\n",
        "    return [pose_predictor(face_image, face_location) for face_location in face_locations]\n",
        "  \n",
        "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
        "    \"\"\"\n",
        "    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n",
        "\n",
        "    :param face_image: image to search\n",
        "    :param face_locations: Optionally provide a list of face locations to check.\n",
        "    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n",
        "    :return: A list of dicts of face feature locations (eyes, nose, etc)\n",
        "    \"\"\"\n",
        "    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n",
        "    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n",
        "\n",
        "    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n",
        "    if model == 'large':\n",
        "        return [{\n",
        "            \"chin\": points[0:17],\n",
        "            \"left_eyebrow\": points[17:22],\n",
        "            \"right_eyebrow\": points[22:27],\n",
        "            \"nose_bridge\": points[27:31],\n",
        "            \"nose_tip\": points[31:36],\n",
        "            \"left_eye\": points[36:42],\n",
        "            \"right_eye\": points[42:48],\n",
        "            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n",
        "            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n",
        "        } for points in landmarks_as_tuples]\n",
        "    elif model == 'small':\n",
        "        return [{\n",
        "            \"nose_tip\": [points[4]],\n",
        "            \"left_eye\": points[2:4],\n",
        "            \"right_eye\": points[0:2],\n",
        "        } for points in landmarks_as_tuples]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZu58XIa_Jdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Retrieve the 128-d vector given facial landmarks and face_image."
      ]
    },
    {
      "metadata": {
        "id": "jObs3kPB5Xyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n",
        "    \"\"\"\n",
        "    Given an image, return the 128-dimension face encoding for each face in the image.\n",
        "\n",
        "    :param face_image: The image that contains one or more faces\n",
        "    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n",
        "    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n",
        "    :return: A list of 128-dimensional face encodings (one for each face in the image)\n",
        "    \"\"\"\n",
        "    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=\"small\")\n",
        "    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MG23_TIa_ncK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _rect_to_css(rect):\n",
        "    \"\"\"\n",
        "    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n",
        "\n",
        "    :param rect: a dlib 'rect' object\n",
        "    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n",
        "    \"\"\"\n",
        "    return rect.top(), rect.right(), rect.bottom(), rect.left()\n",
        "\n",
        "\n",
        "def _css_to_rect(css):\n",
        "    \"\"\"\n",
        "    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\n",
        "\n",
        "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
        "    :return: a dlib `rect` object\n",
        "    \"\"\"\n",
        "    return dlib.rectangle(css[3], css[0], css[1], css[2])\n",
        "\n",
        "\n",
        "def _trim_css_to_bounds(css, image_shape):\n",
        "    \"\"\"\n",
        "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
        "\n",
        "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
        "    :param image_shape: numpy shape of the image array\n",
        "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
        "    \"\"\"\n",
        "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUSf0OFNKuIP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [2] Mount to your google drive... Follow the instructions and paste the auth token."
      ]
    },
    {
      "metadata": {
        "id": "E1mYUBDLt-Xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C48mqQfnK2hg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Navigate to the directory where your images are stored in google drive.\n",
        "\n",
        "This is where I saved my files."
      ]
    },
    {
      "metadata": {
        "id": "7YeB5jzJucWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/College/DL\\ ML/FR_Presentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0tbPnM_vVfq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcs_8W7Iwbue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [3] Modify this code.\n",
        "Ensure that the file names match what is stored in your drive.\n",
        "\"DST.jpg\" and \"JS.jpg\" are the two images I had used in the demo and are stored in my drive.  \n",
        "\"DAJ.jpg\" refers to the image in which I wanted to identify of people that are known."
      ]
    },
    {
      "metadata": {
        "id": "ucTnqFlqsFHF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# This is an example of running face recognition on a single image\n",
        "# and drawing a box around each person that was identified.\n",
        "\n",
        "# Load a sample picture and learn how to recognize it.\n",
        "# This will store the 128-d face encoding of this person for later comparison.\n",
        "# MODIFY THIS LINE TO LOAD YOUR FIRST PERSON\n",
        "dst_image = load_image_file(\"DST.jpg\")\n",
        "dst_face_encoding = face_encodings(dst_image)[0]\n",
        "\n",
        "# Load a second sample picture and learn how to recognize it.\n",
        "# This will store the 128-d face encoding of this person for later comparison.\n",
        "# MODIFY THIS LINE TO LOAD YOUR SECOND PERSON\n",
        "js_image = load_image_file(\"JS.jpg\")\n",
        "js_face_encoding = face_encodings(js_image)[0]\n",
        "\n",
        "# Create arrays of known face encodings and their names\n",
        "# IF YOU ADD ADDITIONAL PEOPLE, BE SURE TO INCLUDE THEM IN THE KNOWN ENCODINGS.\n",
        "known_face_encodings = [\n",
        "    dst_face_encoding,\n",
        "    js_face_encoding\n",
        "]\n",
        "\n",
        "# Specify the names of the persons in the same order as the encoding.\n",
        "# MODIFY THESE NAMES TO MATCH THE PEOPLE YOU JUST ADDED.  NOTE THEY MUST BE IN THE SAME ORDER AS THE KNOWN FACE ENCODINGS.\n",
        "known_face_names = [\n",
        "    \"Daenerys Targaryen\",\n",
        "    \"JonSnow\"\n",
        "]\n",
        "\n",
        "# Load an image to identify faces\n",
        "# MODIFY THIS LINE TO LOAD YOUR IMAGE IN WHICH YOU WANT TO IDENTIFY/RECOGNIZE FACES.\n",
        "unknown_image = load_image_file(\"DAJ.jpg\")\n",
        "\n",
        "# Find all the faces and face encodings in the unknown image\n",
        "list_face_locations = face_locations(unknown_image)\n",
        "list_face_encodings = face_encodings(unknown_image, list_face_locations)\n",
        "\n",
        "# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\n",
        "# See http://pillow.readthedocs.io/ for more about PIL/Pillow\n",
        "pil_image = Image.fromarray(unknown_image)\n",
        "# Create a Pillow ImageDraw Draw instance to draw with\n",
        "draw = ImageDraw.Draw(pil_image)\n",
        "\n",
        "# Loop through each face found in the unknown image\n",
        "for (top, right, bottom, left), face_encoding in zip(list_face_locations, list_face_encodings):\n",
        "    # See if the face is a match for the known face(s)\n",
        "    matches = compare_faces(known_face_encodings, face_encoding)\n",
        "\n",
        "    name = \"Unknown\"\n",
        "\n",
        "    # If a match was found in known_face_encodings, just use the first one.\n",
        "    if True in matches:\n",
        "        first_match_index = matches.index(True)\n",
        "        name = known_face_names[first_match_index]\n",
        "\n",
        "    # Draw a box around the face using the Pillow module\n",
        "    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
        "\n",
        "    # Draw a label with a name below the face\n",
        "    text_width, text_height = draw.textsize(name)\n",
        "    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
        "    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
        "\n",
        "\n",
        "# Remove the drawing library from memory as per the Pillow docs\n",
        "del draw\n",
        "\n",
        "# pil_image.save(\"identified_image.jpg\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atrBroSGsUGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "# Display the resulting image\n",
        "display(pil_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JkLl9b7JsFu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}